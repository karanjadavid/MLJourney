{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Evaluating model performance, we can optimize the models.\n",
    "\n",
    "Recall that we had to choose a value for alpha in ridge and lasso regression before fitting it. Likewise, before fitting and predicting KNN, we choose n_neighbors. \n",
    "\n",
    "Parameters that we specify before fitting a model, like alpha and n_neighbors, are called **hyperparameters**. So, a fundamental step for building a successful model: is choosing the correct hyperparameters. We can try lots of different values, fit all of them separately, see how well they perform, and choose the best values! This is called hyperparameter tuning. \n",
    "\n",
    "When fitting different hyperparameter values, we use cross-validation to avoid overfitting the hyperparameters to the test set. We can still split the data, but perform cross-validation on the training set. We withhold the test set and use it for evaluating the tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach for hyperparameter tuning is called grid search, where we choose a grid of possible hyperparameter values to try. Then we perform k-fold cross-validation for each combination of hyperparameters. We then choose hyperparameters that performed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import Lasso, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnancies</th>\n",
       "      <th>glucose</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>triceps</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>dpf</th>\n",
       "      <th>age</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnancies  glucose  diastolic  triceps  insulin   bmi    dpf  age  \\\n",
       "0            6      148         72       35        0  33.6  0.627   50   \n",
       "1            1       85         66       29        0  26.6  0.351   31   \n",
       "2            8      183         64        0        0  23.3  0.672   32   \n",
       "3            1       89         66       23       94  28.1  0.167   21   \n",
       "4            0      137         40       35      168  43.1  2.288   33   \n",
       "\n",
       "   diabetes  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  \n",
       "4         1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df = pd.read_csv('Data/diabetes_clean.csv')\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned lasso paramaters: {'alpha': 1e-05}\n",
      "Tuned lasso score: 0.33506359955850107\n"
     ]
    }
   ],
   "source": [
    "X = diabetes_df.drop('glucose', axis=1).values\n",
    "y = diabetes_df['glucose'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "lasso = Lasso()\n",
    "\n",
    "param_grid = {'alpha': np.linspace(0.00001, 1, 20)}\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv=kf)\n",
    "\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "print(\"Tuned lasso paramaters: {}\".format(lasso_cv.best_params_))\n",
    "print(\"Tuned lasso score: {}\".format(lasso_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the best model only has an R-squared score of 0.33, highlighting that using the optimal hyperparameters does not guarantee a high performing model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space. In this case, you can use RandomizedSearchCV, which tests a fixed number of hyperparameter settings from specified probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'tol': 0.16334897959183672, 'penalty': 'l2', 'class_weight': 'balanced', 'C': 0.3020408163265306}\n",
      "Tuned Logistic Regression Best Accuracy Score: 0.7633783316026306\n"
     ]
    }
   ],
   "source": [
    "X = diabetes_df.drop('diabetes', axis=1).values\n",
    "y = diabetes_df['diabetes'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "params = {\"penalty\": [\"l1\", \"l2\"], # The regularization penalty options are \"l1\" (Lasso) and \"l2\" (Ridge)\n",
    "         \"tol\": np.linspace(0.0001, 1.0, 50), # tolerance for the change in the optimization objective\n",
    "         \"C\": np.linspace(0.1, 1.0, 50), # The \"C\" parameter is the inverse of regularization strength. It controls the trade-off between fitting the training data well and preventing overfitting. \n",
    "         \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]} # The \"class_weight\" parameter is used to handle class imbalance. It can take two forms: balanced == automatic, manual\n",
    "\n",
    "logreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even without exhaustively trying every combination of hyperparameters, the model has an accuracy of over 70% on the test set!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
